
╭─── Claude Code v2.1.5 ───────────────────────────────────────────────────────╮
│                                                    │ Tips for getting        │
│                Welcome back 충로드!                │ started                 │
│                                                    │ ✔ Run /init to create … │
│                      ▗ ▗   ▖ ▖                     │ ─────────────────────── │
│                                                    │ Recent activity         │
│                        ▘▘ ▝▝                       │ No recent activity      │
│                                                    │                         │
│  Sonnet 4.5 · Claude Pro · prosky1902@gmail.com's  │                         │
│  Organization                                      │                         │
│          ~/Desktop/Project/dw-pipeline-ch          │                         │
╰──────────────────────────────────────────────────────────────────────────────╯

❯ /model 
  ⎿  Set model to sonnet (claude-sonnet-4-5-20250929)

❯ /init 

⏺ I'll analyze the codebase to understand its structure and create a comprehensive CLAUDE.md file.

⏺ Search(pattern: "*")
  ⎿  Found 100 files (ctrl+o to expand)

⏺ Search(pattern: "**/*.md")
  ⎿  Found 0 files 

⏺ Search(pattern: "**/*.py")
  ⎿  Found 7 files (ctrl+o to expand)

⏺ Search(pattern: "**/*.yaml")
  ⎿  Found 1 file (ctrl+o to expand)

⏺ Search(pattern: "**/*.yml")
  ⎿  Found 0 files 

⏺ Read(docker-compose.yaml)
  ⎿  Read 477 lines

⏺ Read(.gitignore)
  ⎿  Read 1 line

⏺ Read(airflow/dags/etl_sales_dag.py)
  ⎿  Read 20 lines

⏺ Read(airflow/dags/etl_marketing_dag.py)
  ⎿  Read 17 lines

⏺ Read(airflow/dags/spark_jobs/spark_submit_example_dag.py)
  ⎿  Read 23 lines

⏺ Read(spark/jobs/krx_api_ingest.py)
  ⎿  Read 50 lines

⏺ Read(airflow/dags/common/utils.py)
  ⎿  Read 20 lines

⏺ Read(airflow/dags/data_quality_check.py)
  ⎿  Read 13 lines

⏺ Search(pattern: "Dockerfile*")
  ⎿  Found 1 file (ctrl+o to expand)

⏺ Search(pattern: "**/*.config")
  ⎿  Found 0 files 

⏺ Read(Dockerfile.spark)
  ⎿  Read 7 lines

⏺ Search(pattern: "trino/**/*")
  ⎿  Found 0 files 

⏺ Search(pattern: "dbt/**/*")
  ⎿  Found 0 files 

⏺ Now I'll create the CLAUDE.md file based on my analysis of the codebase.

⏺ Write(CLAUDE.md)
  ⎿  Wrote 163 lines to CLAUDE.md                                        
      1 # CLAUDE.md
      2 This file provides guidance to Claude Code (claude.ai/code) when 
        working with code in this repository.
      3 ## Overview
      4 This is a data warehouse pipeline project (`dw-pipeline-ch`) that
         integrates Airflow, Spark, MinIO (S3-compatible storage), Hive M
        etastore, Trino, and dbt for ETL/ELT workflows. The primary use c
        ase includes ingesting Korean stock market data from the KRX (Kor
        ea Exchange) API and processing it through a distributed data pip
        eline.
      5 ## Architecture
     … +153 lines (ctrl+o to expand)

⏺ I've created the CLAUDE.md file for your data warehouse pipeline project. The file includes:

  Key sections:
  - Overview: High-level description of the data pipeline architecture
  - Architecture: Detailed breakdown of each layer (Airflow, Spark, Storage, Query) and how they integrate
  - Environment Setup: Commands to start/stop services and access UIs
  - DAG Patterns: How Airflow DAGs are structured with examples
  - Spark Job Development: How to develop and run Spark jobs, including the KRX API pattern
  - Data Flow: End-to-end data movement through the pipeline
  - Configuration Files: Where to find config files for each service
  - Port Reference: Quick lookup table for all service ports
  - Common Issues: Known issues like the Airflow/Trino port conflict

  The documentation focuses on the architectural patterns that require reading multiple files to understand, such as how Airflow submits jobs to Spark, how timezone handling works, and how the KRX API ingestion job dynamically partitions data. It avoids obvious instructions and instead captures the "big picture" knowledge that would help future Claude Code instances be productive quickly.

✻ Baked for 1m 9s

❯ 현재 docker-compose.yaml 파일에서 hive metastore를 이용하여 trino에       
  접속할 수 있게 구성해줘                                                   

⏺ 현재 docker-compose.yaml 파일을 분석해서 Hive Metastore를 통해 Trino에 접속할 수 있도록 구성하겠습니다.

⏺ Update(docker-compose.yaml)
  ⎿  Added 26 lines                                                      
      183        interval: 10s
      184        retries: 5
      185  
      186 +  hive-metastore:                                             
      187 +    image: apache/hive:4.0.0                                  
      188 +    container_name: hive-metastore                            
      189 +    environment:                                              
      190 +      DB_DRIVER: postgres                                     
      191 +      SERVICE_NAME: metastore                                 
      192 +      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=o
          +rg.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:pos
          +tgresql://hive-metastore-db:5432/metastore -Djavax.jdo.option.
          +ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=
          +hive"                                                         
      193 +      # S3 (MinIO) 설정                                       
      194 +      AWS_ACCESS_KEY_ID: minio                                
      195 +      AWS_SECRET_ACCESS_KEY: minio123                         
      196 +      S3_ENDPOINT: http://minio:9000                          
      197 +      S3_PATH_STYLE_ACCESS: "true"                            
      198 +    ports:                                                    
      199 +      - "9083:9083"                                           
      200 +    depends_on:                                               
      201 +      hive-metastore-db:                                      
      202 +        condition: service_healthy                            
      203 +      minio:                                                  
      204 +        condition: service_started                            
      205 +    healthcheck:                                              
      206 +      test: ["CMD", "nc", "-z", "localhost", "9083"]          
      207 +      interval: 10s                                           
      208 +      timeout: 5s                                             
      209 +      retries: 5                                              
      210 +      start_period: 30s                                       
      211 +                                                              
      212    # -------------------------
      213    # Trino
      214    # -------------------------

⏺ Bash(mkdir -p trino/catalog)
  ⎿  (No content)

⏺ Write(trino/catalog/hive.properties)
  ⎿  Wrote 21 lines to trino/catalog/hive.properties   
      1 connector.name=hive
      2 hive.metastore.uri=thrift://hive-metastore:9083
      3 # S3 (MinIO) 설정
      4 hive.s3.endpoint=http://minio:9000
      5 hive.s3.path-style-access=true
      6 hive.s3.aws-access-key=minio
      7 hive.s3.aws-secret-key=minio123
      8 hive.s3.ssl.enabled=false
     … +11 lines (ctrl+o to expand)

⏺ Write(trino/jvm.config)
  ⎿  Wrote 15 lines to trino/jvm.config  
       1 -server
       2 -Xmx2G
       3 -XX:InitialRAMPercentage=80
       4 -XX:MaxRAMPercentage=80
       5 -XX:G1HeapRegionSize=32M
       6 -XX:+ExplicitGCInvokesConcurrent
       7 -XX:+ExitOnOutOfMemoryError
       8 -XX:+HeapDumpOnOutOfMemoryError
       9 -XX:-OmitStackTraceInFastThrow
      10 -XX:ReservedCodeCacheSize=512M
     … +5 lines (ctrl+o to expand)

⏺ Update(docker-compose.yaml)
  ⎿  User rejected update to docker-compose.yaml
      117      entrypoint: ["/opt/spark/bin/spark-class"]
      118      environment:
      119        - GOV_OPEN_API_STOCK_PRICE_SERVICE_KEY=${GOV_OPEN_API_ST
           OCK_PRICE_SERVICE_KEY}
      120 +      # Hive Metastore 설정                                   
      121 +      - HIVE_METASTORE_URIS=thrift://hive-metastore:9083      
      122 +      # S3 (MinIO) 설정                                       
      123 +      - AWS_ACCESS_KEY_ID=minio                               
      124 +      - AWS_SECRET_ACCESS_KEY=minio123                        
      125 +      - AWS_ENDPOINT=http://minio:9000                        
      126      command:
      127        - org.apache.spark.deploy.master.Master
      128      ports:
     ...
      130        - "7077:7077"
      131      volumes:
      132        - ./spark:/opt/spark-apps
      133 +    depends_on:                                               
      134 +      - hive-metastore                                        
      135 +      - minio                                                 
      136  
      137    spark-worker-1:
      138      build:

❯ spark jobs에 국내주식 말고, 해외 주식과 가상자산 시세를 수집하는          
  프로그램도 만들어줘 각각 다른파일로.                                      

⏺ 해외 주식과 가상자산 시세 수집 프로그램을 각각 만들어드리겠습니다.

⏺ Write(spark/jobs/foreign_stock_ingest.py)
  ⎿  Wrote 96 lines to spark/jobs/foreign_stock_ingest.py            
      1 import os
      2 from pyspark.sql import SparkSession
      3 from pyspark.sql.functions import col, lit, current_timestamp
      4 from datetime import datetime, timedelta
      5 import yfinance as yf
      6 import pandas as pd
      7 # 수집할 주요 해외 주식 티커 리스트
      8 TICKERS = [
      9     # 미국 주요 기술주
     … +86 lines (ctrl+o to expand)

⏺ Write(spark/jobs/crypto_ingest.py)
  ⎿  Wrote 130 lines to spark/jobs/crypto_ingest.py                  
      1 import os
      2 import requests
      3 from pyspark.sql import SparkSession
      4 from pyspark.sql.functions import col, lit, current_timestamp
      5 from datetime import datetime
      6 import time
      7 # 수집할 주요 가상자산 목록 (CoinGecko ID 기준)
      8 CRYPTO_IDS = [
      9     'bitcoin', 'ethereum', 'tether', 'binancecoin', 'ripple',
     … +120 lines (ctrl+o to expand)

⏺ Update(Dockerfile.spark)
  ⎿  Added 1 line, removed 1 line                                        
      2  
      3  USER root
      4  RUN apt-get update && apt-get install -y python3-pip
      5 -RUN pip3 install requests                                       
      5 +RUN pip3 install requests yfinance pandas                       
      6  
      7  USER 185
      8   No newline at end of file

⏺ 완료했습니다! 2개의 Spark job 파일을 생성했습니다.

  생성된 파일

  1. 해외 주식 시세 수집 (spark/jobs/foreign_stock_ingest.py)

  - 사용 API: Yahoo Finance (yfinance 라이브러리)
  - 수집 종목:
    - 미국 주요 기술주 (AAPL, MSFT, GOOGL, AMZN, META, TSLA, NVDA, AMD 등)
    - 주요 지수 ETF (SPY, QQQ, DIA)
    - 기타 주요 종목 (NFLX, DIS, PYPL 등)
  - 데이터: 시가, 고가, 저가, 종가, 거래량, 거래소 정보
  - 출력 경로: /opt/spark-apps/output/foreign_stock/YYYYMMDD/

  2. 가상자산 시세 수집 (spark/jobs/crypto_ingest.py)

  - 사용 API: CoinGecko API (무료, API 키 불필요)
  - 수집 코인:
    - 주요 가상자산 (비트코인, 이더리움, 바이낸스코인, 리플 등)
    - 한국 거래소 인기 종목 (Aptos, Sui, SEI 등)
    - 총 24개 코인
  - 데이터:
    - 현재가, 시가총액, 24시간 거래량
    - 24시간 고가/저가, 가격 변동률
    - ATH/ATL, 유통량/총 공급량
  - 출력 경로: /opt/spark-apps/output/crypto/YYYYMMDD/
  - Rate Limit 대응: API 호출 간 6초 대기로 안전하게 처리

  3. Dockerfile.spark 업데이트

  - yfinance, pandas 패키지 추가

  실행 방법

  # Docker 이미지 재빌드 (새 패키지 설치를 위해)
  docker-compose build spark-master spark-worker-1 spark-worker-2

  # 서비스 재시작
  docker-compose up -d

  # 해외 주식 데이터 수집
  docker exec spark-master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    /opt/spark-apps/jobs/foreign_stock_ingest.py

  # 가상자산 데이터 수집
  docker exec spark-master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    /opt/spark-apps/jobs/crypto_ingest.py

  참고사항

  - 해외 주식: Yahoo Finance API는 무료이며 별도 API 키 불필요
  - 가상자산: CoinGecko 무료 API는 분당 10-50회 호출 제한이 있어 Rate Limit 처리 포함
  - 두 job 모두 KRX API와 동일한 패턴(parquet 저장, 동적 파티셔닝)으로 작성

✻ Sautéed for 1m 20s

❯ 이것들도 Airflow DAG 만들어줘 

⏺ 해외 주식과 가상자산 수집을 위한 Airflow DAG를 생성하겠습니다.

⏺ Write(airflow/dags/spark_jobs/foreign_stock_ingest_dag.py)
  ⎿  Wrote 29 lines to airflow/dags/spark_jobs/foreign_stock_ingest_dag.p

      1 from airflow import DAG
      2 from airflow.providers.apache.spark.operators.spark_submit import
         SparkSubmitOperator
      3 from datetime import datetime
      4 import pendulum
      5 kst = pendulum.timezone("Asia/Seoul")
      6 with DAG(
      7     dag_id='foreign_stock_ingest',
      8     start_date=pendulum.datetime(2024, 1, 1, tz=kst),
     … +19 lines (ctrl+o to expand)

⏺ Write(airflow/dags/spark_jobs/crypto_ingest_dag.py)
  ⎿  Wrote 29 lines to airflow/dags/spark_jobs/crypto_ingest_dag.py      
      1 from airflow import DAG
      2 from airflow.providers.apache.spark.operators.spark_submit import
         SparkSubmitOperator
      3 from datetime import datetime
      4 import pendulum
      5 kst = pendulum.timezone("Asia/Seoul")
      6 with DAG(
      7     dag_id='crypto_ingest',
      8     start_date=pendulum.datetime(2024, 1, 1, tz=kst),
     … +19 lines (ctrl+o to expand)

⏺ Write(airflow/dags/spark_jobs/krx_stock_ingest_dag.py)
  ⎿  Wrote 32 lines to airflow/dags/spark_jobs/krx_stock_ingest_dag.py   
      1 from airflow import DAG
      2 from airflow.providers.apache.spark.operators.spark_submit import
         SparkSubmitOperator
      3 from datetime import datetime
      4 import pendulum
      5 kst = pendulum.timezone("Asia/Seoul")
      6 with DAG(
      7     dag_id='krx_stock_ingest',
      8     start_date=pendulum.datetime(2024, 1, 1, tz=kst),
     … +22 lines (ctrl+o to expand)

⏺ 완료했습니다! 3개의 Airflow DAG를 생성했습니다.

  생성된 DAG 파일

  1. 해외 주식 수집 DAG (foreign_stock_ingest_dag.py)

  - DAG ID: foreign_stock_ingest
  - 스케줄: 0 9 * * 1-5 (월~금 오전 9시)
    - 미국 증시 마감 후 데이터 수집
  - 태그: spark, stock, foreign
  - 실행 Job: foreign_stock_ingest.py

  2. 가상자산 수집 DAG (crypto_ingest_dag.py)

  - DAG ID: crypto_ingest
  - 스케줄: 0 */6 * * * (6시간마다 - 00:00, 06:00, 12:00, 18:00)
    - 가상자산은 24시간 거래되므로 하루 4회 수집
  - 태그: spark, crypto, cryptocurrency
  - 실행 Job: crypto_ingest.py

  3. KRX 주식 수집 DAG (krx_stock_ingest_dag.py)

  - DAG ID: krx_stock_ingest
  - 스케줄: 0 18 * * 1-5 (월~금 오후 6시)
    - 한국 증시 마감 후 데이터 수집
  - 태그: spark, stock, krx, korea
  - 실행 Job: krx_api_ingest.py
  - 환경변수: API 키를 Airflow Variable에서 가져옴

  Spark 연결 설정

  DAG를 실행하기 전에 Airflow에서 Spark 연결을 설정해야 합니다:

  # Airflow UI에서 설정하거나, CLI로 설정
  docker exec -it <airflow-container> bash

  # Spark 연결 추가
  airflow connections add 'spark_default' \
      --conn-type 'spark' \
      --conn-host 'spark://spark-master' \
      --conn-port '7077'

  KRX API 키 설정 (krx_stock_ingest DAG용)

  # Airflow Variable로 API 키 등록
  docker exec -it <airflow-container> bash

  airflow variables set GOV_OPEN_API_STOCK_PRICE_SERVICE_KEY "your_api_key_here"

  또는 Airflow UI에서:
  1. Admin → Variables 메뉴
  2. Key: GOV_OPEN_API_STOCK_PRICE_SERVICE_KEY
  3. Value: 실제 API 키 입력

  DAG 확인 및 실행

  # Airflow 재시작하여 새 DAG 인식
  docker-compose restart airflow-scheduler airflow-dag-processor

  # Airflow UI에서 확인
  # http://localhost:8080
  # - foreign_stock_ingest
  # - crypto_ingest  
  # - krx_stock_ingest

  스케줄 요약
  DAG: krx_stock_ingest
  실행 시간: 월~금 18:00 KST
  주기: 주중 매일
  설명: 한국 증시 마감 후
  ────────────────────────────────────────
  DAG: foreign_stock_ingest
  실행 시간: 월~금 09:00 KST
  주기: 주중 매일
  설명: 미국 증시 마감 후
  ────────────────────────────────────────
  DAG: crypto_ingest
  실행 시간: 00:00, 06:00, 12:00, 18:00
  주기: 6시간마다
  설명: 24시간 거래
  모든 DAG는 catchup=False로 설정되어 과거 실행을 건너뜁니다.

✻ Brewed for 57s

❯ /export 
  ⎿  Conversation exported to:
     2026-01-13-local-command-caveatcaveat-the-messages-below-w.txt

❯ claude.md 파일 현재 기준으로 업데이트해줘 

⏺ CLAUDE.md 파일을 현재 프로젝트 상태에 맞게 업데이트하겠습니다.

⏺ Read(CLAUDE.md)
  ⎿  Read 170 lines

⏺ Write(CLAUDE.md)
  ⎿  Added 192 lines, removed 63 lines                                   
      11  
      12  ## Overview
      13  
      14 -This is a data warehouse pipeline project (`dw-pipeline-ch`) th
         -at integrates Airflow, Spark, MinIO (S3-compatible storage), Hi
         -ve Metastore, Trino, and dbt for ETL/ELT workflows. The primary
         - use case includes ingesting Korean stock market data from the 
         -KRX (Korea Exchange) API and processing it through a distribute
         -d data pipeline.                                               
      14 +데이터 웨어하우스 파이프라인 프로젝트(`dw-pipeline-ch`)로, Airf
         +low, Spark, MinIO, Hive Metastore, Trino, dbt를 통합한 ETL/ELT 
         +워크플로우를 제공합니다. 주요 사용 사례는 국내 주식(KRX), 해외 
         +주식(Yahoo Finance), 가상자산(CoinGecko) 시세 데이터를 수집하고
         + 처리하는 분산 데이터 파이프라인입니다.                        
      15  
      16  ## Architecture
      17  
      18  **Orchestration Layer (Airflow)**
      19 -- Airflow manages workflow orchestration using CeleryExecutor w
         -ith Redis and PostgreSQL                                       
      20 -- DAGs are located in `airflow/dags/` with shared utilities in 
         -`airflow/dags/common/`                                         
      21 -- Airflow API server runs on port 8080                         
      19 +- CeleryExecutor with Redis and PostgreSQL로 워크플로우 오케스 
         +트레이션 관리                                                  
      20 +- DAG 위치: `airflow/dags/`, 공통 유틸리티: `airflow/dags/commo
         +n/`                                                            
      21 +- Spark 기반 DAG: `airflow/dags/spark_jobs/`                   
      22 +- Airflow API 서버: port 8080                                  
      23  
      24  **Processing Layer (Spark)**
      24 -- Spark cluster with 1 master and 2 workers (2GB memory, 1 core
         - each)                                                         
      25 -- Master UI: port 8088, Worker UIs: ports 8089-8090            
      26 -- Spark jobs are in `spark/jobs/`, accessible inside containers
         - at `/opt/spark-apps`                                          
      27 -- Custom Spark image extends `apache/spark:3.5.0` with Python p
         -ackages (requests)                                             
      25 +- Spark 클러스터: master 1대 + worker 2대 (각 2GB 메모리, 1 코 
         +어)                                                            
      26 +- Master UI: port 8088, Worker UI: ports 8089-8090             
      27 +- Spark job 위치: `spark/jobs/` → 컨테이너 내 `/opt/spark-apps`
         +로 마운트                                                      
      28 +- 커스텀 이미지: `apache/spark:3.5.0` + Python 패키지 (requests
         +, yfinance, pandas)                                            
      29  
      29 -**Storage Layer**                                              
      30 -- MinIO: S3-compatible object storage on ports 9000 (API) and 9
         -001 (console)                                                  
      31 -- Hive Metastore: PostgreSQL-backed metadata store for table sc
         -hemas                                                          
      32 -- Spark outputs saved to `spark/output/` (e.g., `spark/output/k
         -rx/YYYYMMDD/`)                                                 
      30 +**Storage & Catalog Layer**                                    
      31 +- **MinIO**: S3 호환 객체 스토리지 (API: port 9000, Console: po
         +rt 9001)                                                       
      32 +- **Hive Metastore**: PostgreSQL 기반 메타데이터 저장소 (port 9
         +083)                                                           
      33 +  - Trino와 Spark가 공유하는 테이블 스키마 정보 관리           
      34 +  - MinIO(S3) 연동 설정 포함                                   
      35 +- **로컬 볼륨**: Spark 출력 데이터 저장 (`spark/output/`)      
      36 +  - 국내주식: `spark/output/krx/YYYYMMDD/`                     
      37 +  - 해외주식: `spark/output/foreign_stock/YYYYMMDD/`           
      38 +  - 가상자산: `spark/output/crypto/YYYYMMDD/`                  
      39  
      40  **Query Layer**
      35 -- Trino: Distributed SQL query engine on port 8080 (conflicts w
         -ith Airflow API server port)                                   
      36 -- dbt: Data transformation tool configured to work with Trino  
      41 +- **Trino**: 분산 SQL 쿼리 엔진 (port 8080, Airflow와 포트 충돌
         +)                                                              
      42 +  - Hive Metastore 연동 (`thrift://hive-metastore:9083`)       
      43 +  - MinIO S3 연동 설정 (`trino/catalog/hive.properties`)       
      44 +- **dbt**: Trino 기반 데이터 변환 도구                         
      45  
      46  **Key Integration Points**
      39 -- Airflow can submit Spark jobs using `SparkSubmitOperator` con
         -necting to `spark://spark-master:7077`                         
      40 -- Spark jobs write parquet files to local volumes mounted acros
         -s containers                                                   
      41 -- Environment variable `GOV_OPEN_API_STOCK_PRICE_SERVICE_KEY` i
         -s required for KRX API access                                  
      47 +- Airflow → Spark: `SparkSubmitOperator`로 `spark://spark-maste
         +r:7077` 연결                                                   
      48 +- Spark → Storage: Parquet 파일을 로컬 볼륨에 저장             
      49 +- Spark/Trino → Hive Metastore: 테이블 메타데이터 공유         
      50 +- Hive Metastore → MinIO: S3 스타일 접근으로 객체 스토리지 연동
      51  
      52  ## Environment Setup
      53  
      54  **Prerequisites**
      55  - Docker and Docker Compose
      47 -- Create `.env` file in project root with:                     
      56 +- `.env` 파일 생성 (프로젝트 루트):                            
      57    - `AIRFLOW__CORE__FERNET_KEY`
      58    - `AIRFLOW__API_AUTH__JWT_SECRET`
      50 -  - `GOV_OPEN_API_STOCK_PRICE_SERVICE_KEY` (for KRX API)       
      59 +  - `GOV_OPEN_API_STOCK_PRICE_SERVICE_KEY` (KRX API용)         
      60    - Optional: `AIRFLOW_UID` (default: 50000)
      61  
      62  **Starting Services**
      63  ```bash
      55 -# Start all core services                                      
      64 +# 모든 서비스 시작                                             
      65  docker-compose up -d
      66  
      58 -# Start with optional Flower monitoring                        
      67 +# Flower 모니터링 포함                                         
      68  docker-compose --profile flower up -d
      69  
      61 -# View logs                                                    
      70 +# 로그 확인                                                    
      71  docker-compose logs -f [service-name]
      72  ```
      73  
     ...
       75  - Airflow UI: http://localhost:8080 (user: airflow, pass: airf
           low)
       76  - Spark Master UI: http://localhost:8088
       77  - MinIO Console: http://localhost:9001 (user: minio, pass: min
           io123)
       69 -- Trino: http://localhost:8080 (port conflict with Airflow)   
       78 +- Trino: http://localhost:8080 (Airflow와 포트 충돌 - 동시 사 
          +용 불가)                                                      
       79  
       80  **Stopping Services**
       81  ```bash
       82  docker-compose down
       74 -# With volume cleanup                                         
       83 +# 볼륨 정리 포함                                              
       84  docker-compose down -v
       85  ```
       86  
       87 +**Airflow 초기 설정**                                         
       88 +```bash                                                       
       89 +# Spark 연결 설정                                             
       90 +docker exec -it <airflow-container> airflow connections add 's
          +park_default' \                                               
       91 +    --conn-type 'spark' \                                     
       92 +    --conn-host 'spark://spark-master' \                      
       93 +    --conn-port '7077'                                        
       94 +                                                              
       95 +# KRX API 키 설정 (Airflow Variable)                          
       96 +docker exec -it <airflow-container> airflow variables set \   
       97 +    GOV_OPEN_API_STOCK_PRICE_SERVICE_KEY "your_api_key_here"  
       98 +```                                                           
       99 +                                                              
      100 +## Data Sources                                               
      101 +                                                              
      102 +### 1. 국내 주식 (KRX)                                        
      103 +- **API**: 금융위원회 주식시세정보 API                        
      104 +- **인증**: 환경변수 `GOV_OPEN_API_STOCK_PRICE_SERVICE_KEY` 필
          +요                                                            
      105 +- **수집 주기**: 월~금 18:00 KST (장 마감 후)                 
      106 +- **데이터 지연**: 1-2일 (API 특성상)                         
      107 +- **Job**: `spark/jobs/krx_api_ingest.py`                     
      108 +- **DAG**: `krx_stock_ingest` (airflow/dags/spark_jobs/krx_sto
          +ck_ingest_dag.py)                                             
      109 +                                                              
      110 +### 2. 해외 주식 (Yahoo Finance)                              
      111 +- **API**: yfinance 라이브러리 (Yahoo Finance API)            
      112 +- **인증**: 불필요 (무료)                                     
      113 +- **수집 종목**: 미국 주요 기술주, 지수 ETF 등 20+ 종목       
      114 +- **수집 주기**: 월~금 09:00 KST (미국 장 마감 후)            
      115 +- **Job**: `spark/jobs/foreign_stock_ingest.py`               
      116 +- **DAG**: `foreign_stock_ingest` (airflow/dags/spark_jobs/for
          +eign_stock_ingest_dag.py)                                     
      117 +                                                              
      118 +### 3. 가상자산 (CoinGecko)                                   
      119 +- **API**: CoinGecko API v3                                   
      120 +- **인증**: 불필요 (무료, Rate Limit: 분당 10-50회)           
      121 +- **수집 코인**: 주요 가상자산 24개 (BTC, ETH, BNB, XRP 등)   
      122 +- **수집 주기**: 6시간마다 (00:00, 06:00, 12:00, 18:00 KST)   
      123 +- **Rate Limit 대응**: 배치당 6초 대기                        
      124 +- **Job**: `spark/jobs/crypto_ingest.py`                      
      125 +- **DAG**: `crypto_ingest` (airflow/dags/spark_jobs/crypto_ing
          +est_dag.py)                                                   
      126 +                                                              
      127  ## DAG Patterns
      128  
       80 -**Basic ETL DAG Structure**                                   
       81 -DAGs follow extract → transform → load pattern using `PythonOp
          -erator`:                                                      
       82 -- Sales pipeline: `etl_sales_dag.py` (tag: 'sales')           
       83 -- Marketing pipeline: `etl_marketing_dag.py` (tag: 'marketing'
          -)                                                             
       84 -- Data quality checks: `data_quality_check.py` (tag: 'quality'
          -)                                                             
      129 +**Basic ETL DAG (PythonOperator)**                            
      130 +Extract → Transform → Load 패턴:                              
      131 +- `etl_sales_dag.py` - 매출 데이터 파이프라인 (tag: 'sales')  
      132 +- `etl_marketing_dag.py` - 마케팅 데이터 파이프라인 (tag: 'mar
          +keting')                                                      
      133 +- `data_quality_check.py` - 데이터 품질 점검 (tag: 'quality') 
      134  
       86 -**Spark Job Submission**                                      
       87 -Use `SparkSubmitOperator` with:                               
       88 -- `conn_id="spark_default"` (must be configured in Airflow con
          -nections)                                                     
      135 +**Spark Job Submission DAG**                                  
      136 +`SparkSubmitOperator` 사용 패턴:                              
      137 +- `krx_stock_ingest` - 국내 주식 수집 (월~금 18:00)           
      138 +- `foreign_stock_ingest` - 해외 주식 수집 (월~금 09:00)       
      139 +- `crypto_ingest` - 가상자산 수집 (6시간마다)                 
      140 +                                                              
      141 +필수 설정:                                                    
      142 +- `conn_id="spark_default"` (Airflow Connections에서 설정)    
      143  - `conf={"spark.master": "spark://spark-master:7077"}`
       90 -- Application path: `/opt/airflow/dags/spark_jobs/[job_file].p
          -y`                                                            
      144 +- `application="/opt/spark-apps/jobs/[job_file].py"`          
      145 +- `driver_memory="1g"`, `executor_memory="2g"`, `executor_core
          +s=1`                                                          
      146  
      147  **Timezone Handling**
       93 -Use `pendulum.timezone("Asia/Seoul")` for KST timezone (see `e
          -tl_sales_dag.py`)                                             
      148 +`pendulum.timezone("Asia/Seoul")`로 KST 타임존 설정           
      149  
      150  ## Spark Job Development
      151  
      152  **Job Location**
       98 -Place Spark jobs in `spark/jobs/` - they're mounted to `/opt/s
          -park-apps` in containers                                      
      153 +`spark/jobs/` 디렉토리에 작성 → 컨테이너 내 `/opt/spark-apps/j
          +obs/`로 마운트                                                
      154  
      100 -**KRX API Ingestion Pattern** (`krx_api_ingest.py`)           
      101 -- Fetches stock price data from Korean government API (금융위 
          -원회_주식시세정보API)                                         
      102 -- Uses date offset: `datetime.today() - timedelta(days=2)` for
          - data availability                                            
      103 -- Dynamically calculates partitions based on data size (~128MB
          - per partition)                                               
      104 -- Outputs to `/opt/spark-apps/output/krx/{YYYYMMDD}/` as parqu
          -et                                                            
      155 +**Common Pattern (3개 job 공통)**                             
      156 +1. 외부 API에서 데이터 수집                                   
      157 +2. Pandas DataFrame → Spark DataFrame 변환                    
      158 +3. `collected_at`, `date` 등 메타데이터 컬럼 추가             
      159 +4. 동적 파티셔닝 (target: ~128MB per partition)               
      160 +5. Parquet 형식으로 저장                                      
      161  
      106 -**Environment Variables**                                     
      107 -Access via `os.environ.get("GOV_OPEN_API_STOCK_PRICE_SERVICE_K
          -EY")`                                                         
      162 +**Job Details**                                               
      163  
      109 -**Running Spark Jobs Manually**                               
      164 +*krx_api_ingest.py*                                           
      165 +- 금융위원회 API 호출 (requests)                              
      166 +- 날짜 오프셋: `datetime.today() - timedelta(days=2)`         
      167 +- 출력: `/opt/spark-apps/output/krx/{YYYYMMDD}/`              
      168 +                                                              
      169 +*foreign_stock_ingest.py*                                     
      170 +- yfinance 라이브러리 사용                                    
      171 +- 티커 리스트: AAPL, MSFT, GOOGL, AMZN, META, TSLA, NVDA 등   
      172 +- 최근 5일 데이터 중 최신값 추출                              
      173 +- 출력: `/opt/spark-apps/output/foreign_stock/{YYYYMMDD}/`    
      174 +                                                              
      175 +*crypto_ingest.py*                                            
      176 +- CoinGecko API v3 (requests)                                 
      177 +- 배치 처리 (10개씩) + Rate Limit 대응 (6초 대기)             
      178 +- 시가총액, 거래량, 24시간 변동률 등 수집                     
      179 +- 출력: `/opt/spark-apps/output/crypto/{YYYYMMDD}/`           
      180 +                                                              
      181 +**Running Jobs Manually**                                     
      182  ```bash
      111 -# Via docker exec                                             
      183 +# 국내 주식                                                   
      184  docker exec spark-master /opt/spark/bin/spark-submit \
      185    --master spark://spark-master:7077 \
      186    /opt/spark-apps/jobs/krx_api_ingest.py
      187 +                                                              
      188 +# 해외 주식                                                   
      189 +docker exec spark-master /opt/spark/bin/spark-submit \        
      190 +  --master spark://spark-master:7077 \                        
      191 +  /opt/spark-apps/jobs/foreign_stock_ingest.py                
      192 +                                                              
      193 +# 가상자산                                                    
      194 +docker exec spark-master /opt/spark/bin/spark-submit \        
      195 +  --master spark://spark-master:7077 \                        
      196 +  /opt/spark-apps/jobs/crypto_ingest.py                       
      197  ```
      198  
      199 +**Adding New Dependencies**                                   
      200 +`Dockerfile.spark` 수정 후 이미지 재빌드:                     
      201 +```bash                                                       
      202 +# Dockerfile.spark에 패키지 추가                              
      203 +RUN pip3 install requests yfinance pandas [new-package]       
      204 +                                                              
      205 +# 재빌드                                                      
      206 +docker-compose build spark-master spark-worker-1 spark-worker-
          +2                                                             
      207 +docker-compose up -d                                          
      208 +```                                                           
      209 +                                                              
      210  ## Data Flow
      211  
      119 -1. **Ingestion**: Spark jobs fetch data from external APIs (e.
          -g., KRX)                                                      
      120 -2. **Storage**: Raw data saved as parquet to local volumes (ac
          -cessible across containers)                                   
      121 -3. **Cataloging**: Hive Metastore tracks table schemas        
      122 -4. **Querying**: Trino provides SQL interface over stored data
      123 -5. **Transformation**: dbt models apply business logic        
      124 -6. **Orchestration**: Airflow schedules and monitors entire wo
          -rkflow                                                        
      212 +1. **Ingestion**: Spark job이 외부 API에서 데이터 수집 (KRX, Y
          +ahoo Finance, CoinGecko)                                      
      213 +2. **Storage**: Raw 데이터를 Parquet 형식으로 로컬 볼륨에 저장
      214 +3. **Cataloging**: Hive Metastore가 테이블 스키마 메타데이터  
          +관리                                                          
      215 +4. **Querying**: Trino가 Hive Metastore를 통해 데이터에 SQL 인
          +터페이스 제공                                                 
      216 +5. **Transformation**: dbt 모델이 비즈니스 로직 적용          
      217 +6. **Orchestration**: Airflow가 전체 워크플로우 스케줄링 및 모
          +니터링                                                        
      218  
      219  ## Configuration Files
      220  
      221  **Airflow**
      129 -- Custom config: `airflow/config/airflow.cfg` (mounted via `AI
          -RFLOW_CONFIG` env var)                                        
      222 +- Custom config: `airflow/config/airflow.cfg`                 
      223  - Logs: `airflow/logs/`
      224  - Plugins: `airflow/plugins/`
      225  
      226 +**Spark**                                                     
      227 +- Dockerfile: `Dockerfile.spark`                              
      228 +- Jobs: `spark/jobs/*.py`                                     
      229 +- Output: `spark/output/*/YYYYMMDD/`                          
      230 +                                                              
      231  **Trino**
      134 -- Catalogs: `trino/catalog/`                                  
      135 -- JVM config: `trino/jvm.config`                              
      232 +- Catalog: `trino/catalog/hive.properties`                    
      233 +  - Hive Metastore URI: `thrift://hive-metastore:9083`        
      234 +  - MinIO S3 설정 포함 (endpoint, credentials, path-style-acce
          +ss)                                                           
      235 +- JVM config: `trino/jvm.config` (heap 2GB)                   
      236  
      237 +**Hive Metastore**                                            
      238 +- DB: PostgreSQL (hive-metastore-db)                          
      239 +- Service: Apache Hive 4.0.0 (metastore only)                 
      240 +- Port: 9083 (Thrift)                                         
      241 +                                                              
      242  **dbt**
      138 -- Project root: `dbt/` (working directory in container: `/usr/
          -app`)                                                         
      243 +- Project root: `dbt/` (컨테이너 내 `/usr/app`)               
      244  
      245  ## Port Reference
      246  
     ...
      253  | Spark Worker 2 | 8090 | Web UI |
      254  | MinIO API | 9000 | S3 API |
      255  | MinIO Console | 9001 | Web UI |
      151 -| Trino | 8080 | SQL interface (conflicts with Airflow) |     
      256 +| Hive Metastore | 9083 | Thrift service |                    
      257 +| Trino | 8080 | SQL interface (⚠️ Airflow와 충돌) |          
      258  | Flower | 5555 | Celery monitoring (optional) |
      259  
      260 +## DAG Schedule Summary                                       
      261 +                                                              
      262 +| DAG | 스케줄 | 실행 시간 | 설명 |                           
      263 +|-----|-------|----------|------|                             
      264 +| `krx_stock_ingest` | 월~금 | 18:00 KST | 국내 주식 (장 마감 
          +후) |                                                         
      265 +| `foreign_stock_ingest` | 월~금 | 09:00 KST | 해외 주식 (미국
          + 장 마감 후) |                                                
      266 +| `crypto_ingest` | 매일 | 00:00, 06:00, 12:00, 18:00 KST | 가
          +상자산 (6시간마다) |                                          
      267 +| `etl_sales` | 매일 | @daily | 매출 데이터 ETL |             
      268 +| `etl_marketing` | 매일 | @daily | 마케팅 데이터 ETL |       
      269 +| `data_quality_check` | 매일 | @daily | 데이터 품질 점검 |   
      270 +                                                              
      271 +모든 DAG: `catchup=False` (과거 실행 스킵)                    
      272 +                                                              
      273  ## Common Issues
      274  
      275  **Port Conflicts**
      157 -- Trino and Airflow both use port 8080 - this needs resolution
          - for concurrent use                                           
      276 +- Trino와 Airflow 모두 port 8080 사용 → 동시 실행 불가        
      277 +- 해결: Trino 포트 변경 또는 Airflow API 서버 포트 변경 필요  
      278  
      279  **Data Availability**
      160 -- KRX API typically publishes data with 1-2 day lag (hence `ti
          -medelta(days=2)`)                                             
      280 +- KRX API: 1-2일 지연 (`timedelta(days=2)` 사용)              
      281 +- 해외 주식: Yahoo Finance는 실시간이 아닌 일봉 데이터        
      282 +- 가상자산: CoinGecko API Rate Limit 주의 (무료: 분당 10-50회)
      283  
      284  **Volume Permissions**
      163 -- Airflow requires proper ownership: `chown -R ${AIRFLOW_UID}:
          -0 ./airflow/`                                                 
      285 +- Airflow 볼륨 권한: `chown -R ${AIRFLOW_UID}:0 ./airflow/`   
      286 +- Spark 출력 디렉토리 권한 확인 필요                          
      287  
      165 -**Persistent Data**                                           
      166 -- `postgres-db-volume`: Airflow metadata                      
      167 -- `./hive-metastore-db`: Hive metadata                        
      168 -- `./minio`: Object storage data                              
      169 -- `./spark/output`: Spark job outputs                         
      288 +**Persistent Data Locations**                                 
      289 +- `postgres-db-volume`: Airflow 메타데이터 (Docker volume)    
      290 +- `./hive-metastore-db`: Hive Metastore 메타데이터            
      291 +- `./minio`: MinIO 객체 스토리지 데이터                       
      292 +- `./spark/output`: Spark job 출력 (krx, foreign_stock, crypto
          +)                                                             
      293 +- `./airflow/logs`: Airflow 실행 로그                         
      294 +                                                              
      295 +**API Limitations**                                           
      296 +- CoinGecko: 무료 API는 Rate Limit 존재 (배치 처리로 대응)    
      297 +- Yahoo Finance: 너무 빈번한 요청 시 차단 가능성              
      298 +- KRX API: 서비스 키 발급 필요, 일일 요청 제한 확인           

⏺ CLAUDE.md 파일을 현재 프로젝트 상태에 맞게 업데이트 완료했습니다!

  주요 업데이트 내용

  1. Architecture 섹션 확장

  - Hive Metastore 서비스 추가 (port 9083)
  - Trino와 Hive Metastore 연동 설명
  - 3가지 데이터 출력 경로 명시 (krx, foreign_stock, crypto)
  - Spark 커스텀 이미지 패키지 업데이트 (yfinance, pandas 추가)

  2. Data Sources 새 섹션 추가

  - 국내 주식 (KRX): 금융위원회 API, 월~금 18:00 수집
  - 해외 주식 (Yahoo Finance): yfinance, 월~금 09:00 수집
  - 가상자산 (CoinGecko): 6시간마다 수집, Rate Limit 대응

  3. Airflow 초기 설정 가이드

  - Spark connection 설정 명령어
  - KRX API 키 Variable 설정 방법

  4. DAG Patterns 업데이트

  - Spark Job Submission DAG 패턴 추가
  - 3개 데이터 수집 DAG 설명
  - SparkSubmitOperator 필수 설정 안내

  5. Spark Job Development 상세화

  - 3개 job 공통 패턴 설명
  - 각 job별 상세 내용 (API, 데이터, 출력 경로)
  - 수동 실행 명령어 3가지 모두 제공
  - 새 패키지 추가 방법 가이드

  6. Configuration Files 업데이트

  - Trino catalog 설정 (hive.properties) 설명
  - Hive Metastore 설정 정보
  - 각 서비스별 설정 파일 위치 정리

  7. DAG Schedule Summary 테이블

  - 6개 DAG의 실행 시간과 주기를 한눈에 볼 수 있는 표 추가

  8. Common Issues 확장

  - API Limitations 섹션 추가 (CoinGecko, Yahoo Finance, KRX API)
  - Persistent Data Locations 상세화

  현재 프로젝트의 모든 주요 구성 요소와 3가지 데이터 수집 파이프라인이 CLAUDE.md에 문서화되었습니다!

✻ Worked for 1m 32s

❯ dbt도 셋업하고싶다. dbt는 docker를 따로 띄우지 않고 다른 컨테이너 안에    
  넣는게 낫나? 너는 어떻게 생각해?                                          

⏺ 좋은 질문입니다! dbt 배치 방식에 대해 제 의견을 말씀드리겠습니다.

  제 추천: 별도 컨테이너 + Airflow 통합 하이브리드 방식

  현재 docker-compose.yaml에 이미 dbt 컨테이너가 정의되어 있는데, 이것을 그대로 유지하면서 Airflow에도 dbt를 설치하는 방식을 추천합니다.

  각 방식의 장단점

  1. 별도 dbt 컨테이너 (현재 설정)
  - ✅ 개발/테스트 시 직접 dbt 명령어 실행 편리
  - ✅ dbt 버전 관리 독립적
  - ✅ 책임 분리 명확
  - ❌ Airflow에서 실행 시 DockerOperator 필요 (복잡)

  2. Airflow 컨테이너에 포함
  - ✅ Airflow DAG에서 직접 실행 쉬움 (BashOperator)
  - ✅ 리소스 효율적
  - ❌ Airflow 이미지 커스터마이징 필요
  - ❌ dbt 업데이트 시 Airflow 재빌드

  3. 하이브리드 (추천)
  - ✅ 개발은 별도 dbt 컨테이너에서
  - ✅ 프로덕션 실행은 Airflow에서
  - ✅ 유연성 최대화

  구체적인 구현 방안

  어떤 방식을 원하시나요?
                                                                         
⏺ User answered Claude's questions:
  ⎿  · dbt 셋업을 어떤 방식으로 하시겠습니까? → 하이브리드 (별도 컨테이너 +
      Airflow 통합)


⏺ 좋습니다! 하이브리드 방식으로 진행하겠습니다. dbt 프로젝트를 셋업하고 Airflow와 통합하겠습니다.

⏺ Bash(mkdir -p dbt/models/staging dbt/models/mart dbt/macros dbt/tests)
  ⎿  (No content)

⏺ Write(dbt/dbt_project.yml)
  ⎿  Wrote 30 lines to dbt/dbt_project.yml
      1 name: 'dw_pipeline'
      2 version: '1.0.0'
      3 config-version: 2
      4 profile: 'dw_pipeline'
      5 model-paths: ["models"]
      6 analysis-paths: ["analyses"]
      7 test-paths: ["tests"]
      8 seed-paths: ["seeds"]
     … +20 lines (ctrl+o to expand)

⏺ Write(dbt/profiles.yml)
  ⎿  Wrote 32 lines to dbt/profiles.yml
       1 dw_pipeline:
       2   outputs:
       3     dev:
       4       type: trino
       5       method: none
       6       user: trino
       7       host: trino
       8       port: 8080
       9       database: hive
      10       schema: default
     … +22 lines (ctrl+o to expand)

⏺ Write(dbt/models/staging/stg_krx_stock.sql)
  ⎿  Wrote 42 lines to dbt/models/staging/stg_krx_stock.sql
      1 -- 국내(KRX) 주식 스테이징 모델
      2 -- 금융위원회 API로 수집한 원천 데이터를 정제
      3 {{ config(
      4     materialized='view',
      5     schema='staging'
      6 ) }}
      7 with source_data as (
      8     select
     … +32 lines (ctrl+o to expand)

⏺ Write(dbt/models/staging/stg_foreign_stock.sql)
  ⎿  Wrote 37 lines to dbt/models/staging/stg_foreign_stock.sql
      1 -- 해외 주식 스테이징 모델
      2 -- Yahoo Finance API로 수집한 원천 데이터를 정제
      3 {{ config(
      4     materialized='view',
      5     schema='staging'
      6 ) }}
      7 with source_data as (
      8     select
     … +27 lines (ctrl+o to expand)

⏺ Write(dbt/models/staging/stg_crypto.sql)
  ⎿  Wrote 48 lines to dbt/models/staging/stg_crypto.sql
      1 -- 가상자산 스테이징 모델
      2 -- CoinGecko API로 수집한 원천 데이터를 정제
      3 {{ config(
      4     materialized='view',
      5     schema='staging'
      6 ) }}
      7 with source_data as (
      8     select
     … +38 lines (ctrl+o to expand)

⏺ Write(dbt/models/mart/mart_stock_daily.sql)
  ⎿  Wrote 82 lines to dbt/models/mart/mart_stock_daily.sql              
      1 -- 주식 일별 통합 마트
      2 -- 국내 주식(KRX)과 해외 주식(Yahoo Finance)을 통합하여 일별 주식
         데이터 제공
      3 {{ config(
      4     materialized='table',
      5     schema='mart'
      6 ) }}
      7 with krx_stock as (
      8     select
     … +72 lines (ctrl+o to expand)

⏺ Write(dbt/models/mart/mart_crypto_daily.sql)
  ⎿  Wrote 80 lines to dbt/models/mart/mart_crypto_daily.sql       
      1 -- 가상자산 일별 마트
      2 -- CoinGecko 데이터를 기반으로 일별 가상자산 시장 현황 제공
      3 {{ config(
      4     materialized='table',
      5     schema='mart'
      6 ) }}
      7 with crypto_data as (
      8     select
     … +70 lines (ctrl+o to expand)

⏺ Write(dbt/models/staging/sources.yml)
  ⎿  Wrote 48 lines to dbt/models/staging/sources.yml         
      1 version: 2
      2 sources:
      3   - name: raw
      4     description: "Spark job으로 수집된 원천 데이터"
      5     database: hive
      6     schema: default
      7     tables:
      8       - name: krx_stock_raw
      9         description: "국내(KRX) 주식 시세 원천 데이터"
     … +38 lines (ctrl+o to expand)

⏺ Write(dbt/models/mart/schema.yml)
  ⎿  Wrote 39 lines to dbt/models/mart/schema.yml            
      1 version: 2
      2 models:
      3   - name: mart_stock_daily
      4     description: "국내 및 해외 주식 일별 통합 데이터"
      5     columns:
      6       - name: symbol
      7         description: "주식 코드 또는 티커"
      8         tests:
      9           - not_null
     … +29 lines (ctrl+o to expand)

⏺ Write(dbt/README.md)
  ⎿  Wrote 95 lines to dbt/README.md                  
      1 # dbt 프로젝트: dw_pipeline
      2 데이터 웨어하우스 파이프라인의 dbt 변환 레이어
      3 ## 프로젝트 구조
      4 ```
      5 dbt/
      6 ├── dbt_project.yml      # 프로젝트 설정
      7 ├── profiles.yml         # Trino 연결 설정
     … +85 lines (ctrl+o to expand)

⏺ Read(docker-compose.yaml)
  ⎿  Read 10 lines

⏺ Update(docker-compose.yaml)
  ⎿  Added 2 lines, removed 1 line                                       
      70      AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
      71      # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for
           a quick checks
      72      # for other purpose (development, test and especially produ
          ction usage) build/extend Airflow image.
      73 -    # _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREME
         -NTS}                                                           
      73 +    _PIP_ADDITIONAL_REQUIREMENTS: 'dbt-trino==1.8.0'           
      74      # The following line can be used to set a custom config fil
          e, stored in the local config folder
      75      AIRFLOW_CONFIG: '/opt/airflow/config/airflow.cfg'
      76      AIRFLOW__LOGGING__WORKER_LOG_SERVER_HOST: "localhost"
     ...
      79      - ./airflow/logs:/opt/airflow/logs
      80      - ./airflow/config:/opt/airflow/config
      81      - ./airflow/plugins:/opt/airflow/plugins
      82 +    - ./dbt:/opt/dbt                                           
      83    user: "${AIRFLOW_UID:-50000}:0"
      84    depends_on:
      85      &airflow-common-depends-on

⏺ Write(airflow/dags/dbt_transform_dag.py)
  ⎿  Wrote 53 lines to airflow/dags/dbt_transform_dag.py
      1 from airflow import DAG
      2 from airflow.operators.bash import BashOperator
      3 from datetime import datetime
      4 import pendulum
      5 kst = pendulum.timezone("Asia/Seoul")
      6 # dbt 프로젝트 경로
      7 DBT_PROJECT_DIR = '/opt/dbt'
      8 DBT_PROFILES_DIR = '/opt/dbt'
     … +43 lines (ctrl+o to expand)

⏺ Write(airflow/dags/master_pipeline_dag.py)
  ⎿  Wrote 34 lines to airflow/dags/master_pipeline_dag.py               
      1 from airflow import DAG
      2 from airflow.operators.trigger_dagrun import TriggerDagRunOperato
        r
      3 from airflow.operators.empty import EmptyOperator
      4 from datetime import datetime
      5 import pendulum
      6 kst = pendulum.timezone("Asia/Seoul")
      7 with DAG(
      8     dag_id='master_pipeline',
     … +24 lines (ctrl+o to expand)

⏺ Read(.gitignore)
  ⎿  Read 2 lines

⏺ Update(.gitignore)
  ⎿  Added 5 lines, removed 1 line                                       
      1  .env
      2 -CLAUDE.md                                                       
      2   No newline at end of file
      3 +                                                                
      4 +# dbt                                                           
      5 +dbt/target/                                                     
      6 +dbt/dbt_packages/                                               
      7 +dbt/logs/                                                       
      8   No newline at end of file

⏺ 완료했습니다! dbt를 하이브리드 방식으로 셋업했습니다.

  생성된 파일 및 구조

  1. dbt 프로젝트 구조

  dbt/
  ├── dbt_project.yml           # 프로젝트 설정
  ├── profiles.yml              # Trino 연결 설정
  ├── README.md                 # 사용 가이드
  ├── models/
  │   ├── staging/              # Staging Layer (View)
  │   │   ├── sources.yml       # 원천 데이터 정의
  │   │   ├── stg_krx_stock.sql
  │   │   ├── stg_foreign_stock.sql
  │   │   └── stg_crypto.sql
  │   └── mart/                 # Mart Layer (Table)
  │       ├── schema.yml        # 테스트 및 문서화
  │       ├── mart_stock_daily.sql
  │       └── mart_crypto_daily.sql
  ├── macros/
  └── tests/

  2. dbt 모델 설명

  Staging Models (View)
  - stg_krx_stock: 국내 주식 데이터 정제
  - stg_foreign_stock: 해외 주식 데이터 정제
  - stg_crypto: 가상자산 데이터 정제

  Mart Models (Table)
  - mart_stock_daily: 국내+해외 주식 통합 (등락률, 거래량 등)
  - mart_crypto_daily: 가상자산 시장 데이터 (시가총액, 변동률 등)

  3. Airflow 통합

  - docker-compose.yaml: dbt-trino 패키지 추가, dbt 볼륨 마운트
  - dbt_transform_dag.py: dbt 실행 DAG (매일 20:00)
    - dbt debug → deps → staging → mart → test
  - master_pipeline_dag.py: 전체 파이프라인 오케스트레이션 (매일 21:00)

  실행 방법

  초기 셋업

  # Airflow 컨테이너 재시작 (dbt-trino 설치)
  docker-compose down
  docker-compose up -d

  # dbt 컨테이너에서 연결 테스트
  docker exec -it dbt bash
  cd /usr/app
  dbt debug

  개발 시 (별도 dbt 컨테이너)

  docker exec -it dbt bash

  # 모든 모델 실행
  dbt run

  # Staging만 실행
  dbt run --select staging.*

  # Mart만 실행
  dbt run --select mart.*

  # 특정 모델만 실행
  dbt run --select mart_stock_daily

  # 테스트 실행
  dbt test

  # 문서 생성
  dbt docs generate

  프로덕션 (Airflow)

  - Airflow UI에서 dbt_transform DAG 활성화
  - 매일 20:00에 자동 실행
  - 또는 master_pipeline DAG로 전체 파이프라인 실행 (21:00)

  데이터 플로우

  1. 데이터 수집 (Spark Jobs)
     ├─ krx_stock_ingest (18:00) → spark/output/krx/
     ├─ foreign_stock_ingest (09:00) → spark/output/foreign_stock/
     └─ crypto_ingest (6시간마다) → spark/output/crypto/

  2. Hive Metastore에 테이블 등록
     └─ Trino가 Parquet 파일을 테이블로 인식

  3. dbt 변환 (20:00)
     ├─ Staging: 원천 데이터 정제 (View)
     │   ├─ stg_krx_stock
     │   ├─ stg_foreign_stock
     │   └─ stg_crypto
     └─ Mart: 비즈니스 로직 적용 (Table)
         ├─ mart_stock_daily (국내+해외 통합)
         └─ mart_crypto_daily

  4. 데이터 활용
     └─ Trino SQL로 mart 테이블 조회

  주요 설정

  profiles.yml

  - Database: hive (Trino catalog)
  - Schema: default (raw), staging, mart
  - Host: trino:8080
  - Target: dev (개발), prod (프로덕션)

  dbt_project.yml

  - Staging: View로 생성 (schema: staging)
  - Mart: Table로 생성 (schema: mart)

  다음 단계

  1. Spark job에서 Hive 테이블 생성 추가
    - 현재는 Parquet 파일만 저장
    - Hive Metastore에 테이블 등록 필요
  2. dbt 모델 확장
    - 시계열 분석 모델
    - 종목별 성과 분석 모델
    - 포트폴리오 추천 모델
  3. BI 도구 연동
    - Trino를 통해 Tableau, Superset 등 연동
    - 실시간 대시보드 구축

  하이브리드 방식으로 개발 편의성과 프로덕션 안정성을 모두 확보했습니다! 🎉

✻ Cooked for 3m 40s



